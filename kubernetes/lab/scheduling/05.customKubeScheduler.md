# Custom scheduler

## Описание.
В этой лабораторной создадим кастомный kube scheduler. Запустим его на воркер ноде. А потом зашедулим с его помощью под на ноду кластера.

## Поехали!

### Скачаем официальную репу Kubernetes и скомпилируем бинарники.

Я, для примера, выкачиваю версию 1.30.
```
git clone -b release-1.30 --single-branch https://github.com/kubernetes/kubernetes.git
cd kubernetes
make
```

### Создадим новый имидж на основе busybox и засунем туда бинарник scheduler.
Бинарник scheduler получаем на предыдущем этапе. Он будет лежать по следующему пути: _output/local/bin/linux/amd64/kube-scheduler

Создаём Dockerfile. И добавляем в образ всё содержимое текущего каталога. Там должен лежать бинарник kube-scheduler.

vi Dockerfile 

```
FROM busybox
ADD . /usr/local/bin
```

```
docker build -t cr.yandex/crp59ldu2qv9q43uq5jh/cameda-kub-scheduler:2.0 -f Dockerfile .
docker push cr.yandex/crp59ldu2qv9q43uq5jh/cameda-kub-scheduler:2.0
```
Вместо crp59ldu2qv9q43uq5jh укажите свой реджистри. Также на свой реджистри поменяйте настройки в Deployment. Ниже по тексту.

### Создадим кастомный Scheduler на одной из воркер нод.
```
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cam-scheduler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cam-scheduler-as-kube-scheduler
subjects:
- kind: ServiceAccount
  name: cam-scheduler
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: system:kube-scheduler
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cam-scheduler-as-volume-scheduler
subjects:
- kind: ServiceAccount
  name: cam-scheduler
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: system:volume-scheduler
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cam-scheduler-extension-apiserver-authentication-reader
  namespace: kube-system
roleRef:
  kind: Role
  name: extension-apiserver-authentication-reader
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: cam-scheduler
  namespace: kube-system
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: cam-scheduler-config
  namespace: kube-system
data:
  cam-scheduler-config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1
    kind: KubeSchedulerConfiguration
    profiles:
      - schedulerName: cam-scheduler
    leaderElection:
      leaderElect: false    
EOF
```
--------------------------------------------------------------------------------
```
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    component: scheduler
    tier: control-plane
  name: cam-scheduler
  namespace: kube-system
spec:
  selector:
    matchLabels:
      component: scheduler
      tier: control-plane
  replicas: 1
  template:
    metadata:
      labels:
        component: scheduler
        tier: control-plane
        version: second
    spec:
      serviceAccountName: cam-scheduler
      containers:
      - command:
        - /usr/local/bin/kube-scheduler
        - --config=/etc/kubernetes/cam-scheduler/cam-scheduler-config.yaml
        image: cr.yandex/crp59ldu2qv9q43uq5jh/cameda-kub-scheduler:2.0
        livenessProbe:
          httpGet:
            path: /healthz
            port: 10259
            scheme: HTTPS
          initialDelaySeconds: 15
        name: cam-kube-second-scheduler
        readinessProbe:
          httpGet:
            path: /healthz
            port: 10259
            scheme: HTTPS
        resources:
          requests:
            cpu: '0.1'
        securityContext:
          privileged: false
        volumeMounts:
          - name: config-volume
            mountPath: /etc/kubernetes/cam-scheduler
      hostNetwork: false
      hostPID: false
      volumes:
        - name: config-volume
          configMap:
            name: cam-scheduler-config
EOF
```

## Создадим тестовый под, который зашедулим с помощью нового шедулера - cam-scheduler.
```
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: cam-pod-busybox
  namespace: default
  labels:
    cam: busybox
  annotations:
    author: cameda
spec:
  containers:
  - name: busybox
    image: busybox
    imagePullPolicy: IfNotPresent
    resources:
      requests:
        cpu: "100m"
        memory: "120Mi"
      limits:
        memory: "200Mi"
    command: ["sh", "-c"]
    args: ["sleep 24h"]
  restartPolicy: OnFailure
  hostname: busybox
  schedulerName: cam-scheduler
  nodeSelector:
    kubernetes.io/os: linux
EOF
```

### Результат.
```
kubectl get po | grep cam-pod-busybox
cam-pod-busybox                                  1/1     Running   0          22s
```
